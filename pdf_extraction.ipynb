{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529f34de-a69b-4825-87d8-f7b6a54bc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New notebook cell - PDF processing\n",
    "import requests\n",
    "import PyPDF2\n",
    "import openai\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url):\n",
    "    \"\"\"Download PDF and extract text\"\"\"\n",
    "    # Download PDF\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    # Read PDF content\n",
    "    pdf_file = BytesIO(response.content)\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap  # Overlap for continuity\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "def process_article_fully(article_id):\n",
    "    \"\"\"Take an article from metadata_only to fully_processed\"\"\"\n",
    "    # 1. Get article info\n",
    "    article = supabase.table('articles').select('*').eq('id', article_id).execute()\n",
    "    article = article.data[0]\n",
    "    \n",
    "    # 2. Extract PDF text\n",
    "    text = extract_text_from_pdf_url(article['pdf_url'])\n",
    "    \n",
    "    # 3. Chunk text\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # 4. Process each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Generate embedding\n",
    "        embedding = openai.Embedding.create(\n",
    "            input=chunk,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )['data'][0]['embedding']\n",
    "        \n",
    "        # Store chunk\n",
    "        chunk_data = {\n",
    "            'article_id': article_id,\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_index': i,\n",
    "            'embedding': embedding\n",
    "        }\n",
    "        supabase.table('article_chunks').insert(chunk_data).execute()\n",
    "    \n",
    "    # 5. Update article status\n",
    "    supabase.table('articles').update({'processing_status': 'fully_processed'}).eq('id', article_id).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e674da3c-0460-4afd-b5a4-0e0822a7032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/1905.11833\"\n",
    "x = extract_text_from_pdf_url(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9987780-3c85-4314-a0dd-f31555eaed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61036"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f58b95-52cc-403d-a0a7-b2bd86b5df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interpreting and improving natural-language\\nprocessing (in machines) with natural\\nlanguage-processing (in the brain)\\nMariya Toneva\\nNeuroscience Instit'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:150].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d717aa9e-b0ca-4b22-8c02-b8423b9fe319",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = chunk_text(x[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a64ca5-f58c-4132-9a66-ab48b8752118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eef50dc-6de3-426b-bcf1-d98717e854d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Interpreting and improving natural-language\\nprocessing (in machines) with natural\\nlanguage-processing (in the brain)\\nMariya Toneva\\nNeuroscience Institute\\nDepartment of Machine Learning\\nCarnegie Mellon University\\nmariya@cmu.eduLeila Wehbe\\nNeuroscience Institute\\nDepartment of Machine Learning\\nCarnegie Mellon University\\nlwehbe@cmu.edu\\nAbstract\\nNeural networks models for NLP are typically implemented without the explicit\\nencoding of language rules and yet they are able to break one performance record\\nafter another. This has generated a lot of research interest in interpreting the\\nrepresentations learned by these networks. We propose here a novel interpretation\\napproach that relies on the only processing system we have that does understand\\nlanguage: the human brain. We use brain imaging recordings of subjects reading\\ncomplex natural text to interpret word and sequence embeddings from 4recent\\nNLP models - ELMo, USE, BERT and Transformer-XL. We study how their\\nrepresentations differ across la',\n",
       " '...',\n",
       " '...',\n",
       " 'ngs of subjects reading\\ncomplex natural text to interpret word and sequence embeddings from 4recent\\nNLP models - ELMo, USE, BERT and Transformer-XL. We study how their\\nrepresentations differ across layer depth, context length, and attention type. Our\\nresults reveal differences in the context-related representations across these models.\\nFurther, in the transformer models, we ﬁnd an interaction between layer depth and\\ncontext length, and between layer depth and attention type. We ﬁnally hypothesize\\nthat altering BERT to better align with brain recordings would enable it to also\\nbetter understand language. Probing the altered BERT using syntactic NLP tasks\\nreveals that the model with increased ')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0], \"...\", \"...\", y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6982f03-1e67-409c-a852-5595f837c3c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m embedding = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-ada-002\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Code\\ai-research-tracking\\.venv\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "embedding = openai.Embedding.create(input=y[0], model=\"text-embedding-ada-002\")['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2a6298-0f61-4512-8886-2208dd5aa840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated embedding with 1536 dimensions\n",
      "First 5 values: [-0.025302225723862648, -0.0005367214907892048, -0.0003945132193621248, 0.0063355653546750546, -0.0010434165596961975]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Test with your chunk\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding using new OpenAI API\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Test it\n",
    "test_text = \"This is a test sentence for embedding generation.\"\n",
    "embedding = generate_embedding(test_text)\n",
    "\n",
    "print(f\"✅ Generated embedding with {len(embedding)} dimensions\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa7a225-93c6-440a-85ab-db9defbd9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding using OpenAI\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54ac8df-5f6d-40aa-b341-87387e6069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_embedding(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f02813-abb1-4e89-8478-c65931184d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03363104164600372,\n",
       " 0.009249209426343441,\n",
       " 0.023493262007832527,\n",
       " -0.03346948325634003,\n",
       " 0.00556029612198472,\n",
       " 0.012776564806699753,\n",
       " 0.010097390040755272,\n",
       " 0.01740790158510208,\n",
       " -0.02598395198583603,\n",
       " -0.04103579372167587]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6590ce9f-40db-4267-97a9-272237cfd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a paper to process...\n",
      "Found paper: BrainMAP: Learning Multiple Activation Pathways in Brain Net...\n",
      "\n",
      "============================================================\n",
      "Processing article: e770c938-53e0-46d6-b146-41f4ad3839a9\n",
      "============================================================\n",
      "📄 Title: BrainMAP: Learning Multiple Activation Pathways in Brain Networks\n",
      "\n",
      "📥 Extracting PDF text...\n",
      "Downloading PDF from: http://arxiv.org/pdf/2412.17404v2\n",
      "  Extracted page 1/11\n",
      "  Extracted page 2/11\n",
      "  Extracted page 3/11\n",
      "  Extracted page 4/11\n",
      "  Extracted page 5/11\n",
      "  Extracted page 6/11\n",
      "  Extracted page 7/11\n",
      "  Extracted page 8/11\n",
      "  Extracted page 9/11\n",
      "  Extracted page 10/11\n",
      "  Extracted page 11/11\n",
      "✅ Extracted 52439 characters\n",
      "\n",
      "✂️  Chunking text...\n",
      "✅ Created 66 chunks\n",
      "\n",
      "🔄 Generating embeddings and storing chunks...\n",
      "  Processing chunk 61/66..."
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'message': 'unsupported Unicode escape sequence', 'code': '22P05', 'hint': None, 'details': '\\\\u0000 cannot be converted to text.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Process it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m num_chunks = \u001b[43mprocess_article_fully\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Verify chunks were stored\u001b[39;00m\n\u001b[32m    125\u001b[39m chunks_result = supabase.table(\u001b[33m'\u001b[39m\u001b[33marticle_chunks\u001b[39m\u001b[33m'\u001b[39m)\\\n\u001b[32m    126\u001b[39m     .select(\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m)\\\n\u001b[32m    127\u001b[39m     .eq(\u001b[33m'\u001b[39m\u001b[33marticle_id\u001b[39m\u001b[33m'\u001b[39m, paper[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m])\\\n\u001b[32m    128\u001b[39m     .execute()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mprocess_article_fully\u001b[39m\u001b[34m(article_id)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Store in database\u001b[39;00m\n\u001b[32m     80\u001b[39m     chunk_data = {\n\u001b[32m     81\u001b[39m         \u001b[33m'\u001b[39m\u001b[33marticle_id\u001b[39m\u001b[33m'\u001b[39m: article_id,\n\u001b[32m     82\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m'\u001b[39m: chunk,\n\u001b[32m     83\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m'\u001b[39m: i,\n\u001b[32m     84\u001b[39m         \u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m: embedding\n\u001b[32m     85\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[43msupabase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43marticle_chunks\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Stored \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 5. Update article status\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Code\\ai-research-tracking\\.venv\\Lib\\site-packages\\postgrest\\_sync\\request_builder.py:78\u001b[39m, in \u001b[36mSyncQueryRequestBuilder.execute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         json_obj = model_validate_json(APIErrorFromJSON, r.content)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;28mdict\u001b[39m(json_obj))\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(generate_default_error_message(r))\n",
      "\u001b[31mAPIError\u001b[39m: {'message': 'unsupported Unicode escape sequence', 'code': '22P05', 'hint': None, 'details': '\\\\u0000 cannot be converted to text.'}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import supabase\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url):\n",
    "    \"\"\"Download PDF and extract text\"\"\"\n",
    "    print(f\"Downloading PDF from: {pdf_url}\")\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    pdf_file = BytesIO(response.content)\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(pdf_reader.pages):\n",
    "        text += page.extract_text()\n",
    "        print(f\"  Extracted page {page_num + 1}/{len(pdf_reader.pages)}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding using OpenAI\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def process_article_fully(article_id):\n",
    "    \"\"\"Process an article: extract PDF, chunk, embed, store\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing article: {article_id}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # 1. Get article from database\n",
    "    result = supabase.table('articles').select('*').eq('id', article_id).execute()\n",
    "    if not result.data:\n",
    "        print(\"❌ Article not found\")\n",
    "        return\n",
    "    \n",
    "    article = result.data[0]\n",
    "    print(f\"📄 Title: {article['title']}\")\n",
    "    \n",
    "    # 2. Extract text from PDF\n",
    "    print(\"\\n📥 Extracting PDF text...\")\n",
    "    text = extract_text_from_pdf_url(article['pdf_url'])\n",
    "    print(f\"✅ Extracted {len(text)} characters\")\n",
    "    \n",
    "    # 3. Chunk the text\n",
    "    print(\"\\n✂️  Chunking text...\")\n",
    "    chunks = chunk_text(text, chunk_size=1000, overlap=200)\n",
    "    print(f\"✅ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # 4. Process each chunk\n",
    "    print(\"\\n🔄 Generating embeddings and storing chunks...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  Processing chunk {i+1}/{len(chunks)}...\", end='\\r')\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = generate_embedding(chunk)\n",
    "        \n",
    "        # Store in database\n",
    "        chunk_data = {\n",
    "            'article_id': article_id,\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_index': i,\n",
    "            'embedding': embedding\n",
    "        }\n",
    "        \n",
    "        supabase.table('article_chunks').insert(chunk_data).execute()\n",
    "    \n",
    "    print(f\"\\n✅ Stored {len(chunks)} chunks\")\n",
    "    \n",
    "    # 5. Update article status\n",
    "    print(\"\\n📝 Updating article status...\")\n",
    "    supabase.table('articles')\\\n",
    "        .update({'processing_status': 'fully_processed'})\\\n",
    "        .eq('id', article_id)\\\n",
    "        .execute()\n",
    "    \n",
    "    print(\"✅ Article fully processed!\")\n",
    "    return len(chunks)\n",
    "\n",
    "from supabase import create_client, Client\n",
    "load_dotenv()\n",
    "\n",
    "# You'll get these from Supabase dashboard\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\", \"\")\n",
    "SUPABASE_KEY = os.environ.get(\"SUPABASE_KEY\", \"\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# Test with one of your saved papers\n",
    "print(\"Getting a paper to process...\")\n",
    "result = supabase.table('articles')\\\n",
    "    .select('*')\\\n",
    "    .eq('processing_status', 'metadata_only')\\\n",
    "    .limit(1)\\\n",
    "    .execute()\n",
    "\n",
    "if result.data:\n",
    "    paper = result.data[0]\n",
    "    print(f\"Found paper: {paper['title'][:60]}...\")\n",
    "    \n",
    "    # Process it\n",
    "    num_chunks = process_article_fully(paper['id'])\n",
    "    \n",
    "    # Verify chunks were stored\n",
    "    chunks_result = supabase.table('article_chunks')\\\n",
    "        .select('*')\\\n",
    "        .eq('article_id', paper['id'])\\\n",
    "        .execute()\n",
    "    \n",
    "    print(f\"\\n📊 Verification: {len(chunks_result.data)} chunks in database\")\n",
    "else:\n",
    "    print(\"No unprocessed papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47bc486e-c344-4cde-b748-9703a1145133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove problematic characters from text\"\"\"\n",
    "    # Remove null bytes\n",
    "    text = text.replace('\\x00', '')\n",
    "    \n",
    "    # Remove other control characters except newlines and tabs\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F-\\x9F]', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    # Clean text first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Clean each chunk as well\n",
    "        chunk = clean_text(chunk)\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55cea72c-8e61-4bc9-982e-a5ad65eeb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a paper to process...\n",
      "Found paper: BrainMAP: Learning Multiple Activation Pathways in Brain Net...\n",
      "\n",
      "============================================================\n",
      "Processing article: e770c938-53e0-46d6-b146-41f4ad3839a9\n",
      "============================================================\n",
      "📄 Title: BrainMAP: Learning Multiple Activation Pathways in Brain Networks\n",
      "\n",
      "📥 Extracting PDF text...\n",
      "Downloading PDF from: http://arxiv.org/pdf/2412.17404v2\n",
      "  Extracted page 1/11\n",
      "  Extracted page 2/11\n",
      "  Extracted page 3/11\n",
      "  Extracted page 4/11\n",
      "  Extracted page 5/11\n",
      "  Extracted page 6/11\n",
      "  Extracted page 7/11\n",
      "  Extracted page 8/11\n",
      "  Extracted page 9/11\n",
      "  Extracted page 10/11\n",
      "  Extracted page 11/11\n",
      "✅ Extracted 52439 characters\n",
      "\n",
      "✂️  Chunking text...\n",
      "✅ Created 66 chunks\n",
      "\n",
      "🔄 Generating embeddings and storing chunks...\n",
      "  Processing chunk 66/66...\n",
      "✅ Stored 66 chunks\n",
      "\n",
      "📝 Updating article status...\n",
      "✅ Article fully processed!\n",
      "\n",
      "📊 Verification: 126 chunks in database\n"
     ]
    }
   ],
   "source": [
    "# Test with one of your saved papers\n",
    "print(\"Getting a paper to process...\")\n",
    "result = supabase.table('articles')\\\n",
    "    .select('*')\\\n",
    "    .eq('processing_status', 'metadata_only')\\\n",
    "    .limit(1)\\\n",
    "    .execute()\n",
    "\n",
    "if result.data:\n",
    "    paper = result.data[0]\n",
    "    print(f\"Found paper: {paper['title'][:60]}...\")\n",
    "    \n",
    "    # Process it\n",
    "    num_chunks = process_article_fully(paper['id'])\n",
    "    \n",
    "    # Verify chunks were stored\n",
    "    chunks_result = supabase.table('article_chunks')\\\n",
    "        .select('*')\\\n",
    "        .eq('article_id', paper['id'])\\\n",
    "        .execute()\n",
    "    \n",
    "    print(f\"\\n📊 Verification: {len(chunks_result.data)} chunks in database\")\n",
    "else:\n",
    "    print(\"No unprocessed papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c9878-cd75-4f71-9a6e-f14245e5f07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
