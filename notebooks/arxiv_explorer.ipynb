{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a75382f-5c3d-4ef1-a795-89bde7a8b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Current time: 2025-09-20 14:28:04.504664\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timedelta\n",
    "import arxiv\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344beeef-017f-4f72-83e8-688f501cd61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making request to arXiv API...\n",
      "URL: http://export.arxiv.org/api/query\n",
      "Parameters: {'search_query': 'cat:cs.AI', 'start': 0, 'max_results': 3, 'sortBy': 'submittedDate', 'sortOrder': 'descending'}\n",
      "\n",
      "Response status: 200\n",
      "Response headers: {'Connection': 'keep-alive', 'Content-Length': '2906', 'content-encoding': 'gzip', 'server': 'Apache', 'content-type': 'application/atom+xml; charset=UTF-8', 'via': '1.1 varnish, 1.1 varnish, 1.1 varnish', 'access-control-allow-origin': '*', 'Accept-Ranges': 'bytes', 'Age': '0', 'Date': 'Sat, 20 Sep 2025 13:28:29 GMT', 'X-Served-By': 'cache-lga21958-LGA, cache-lga21971-LGA, cache-man4136-MAN', 'X-Cache': 'MISS, MISS, MISS', 'X-Cache-Hits': '0, 0, 0', 'X-Timer': 'S1758374909.921910,VS0,VE276', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=300'}\n",
      "Response length: 8883 bytes\n"
     ]
    }
   ],
   "source": [
    "# Make a raw HTTP request to arXiv API first\n",
    "base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "# Simple search for AI papers\n",
    "params = {\n",
    "    'search_query': 'cat:cs.AI',\n",
    "    'start': 0,\n",
    "    'max_results': 3,\n",
    "    'sortBy': 'submittedDate',\n",
    "    'sortOrder': 'descending'\n",
    "}\n",
    "\n",
    "print(\"Making request to arXiv API...\")\n",
    "print(f\"URL: {base_url}\")\n",
    "print(f\"Parameters: {params}\")\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "\n",
    "print(f\"\\nResponse status: {response.status_code}\")\n",
    "print(f\"Response headers: {dict(response.headers)}\")\n",
    "print(f\"Response length: {len(response.content)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5469753f-40f2-43e3-97aa-41aa0b72ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw XML response (first 1000 characters):\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D3\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=3</title>\n",
      "  <id>http://arxiv.org/api/R0SPnx8l8tIxrHMPwcpdin6AZn8</id>\n",
      "  <updated>2025-09-20T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">142084</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">3</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2509.15217v1</id>\n",
      "    <updated>2025-09-18T17:59:11Z</updated>\n",
      "    <published>2025-09-18T17:59:11Z</published>\n",
      "    <title>Generalizable Geometric Image Caption Synthesis</title>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Root tag: {http://www.w3.org/2005/Atom}feed\n",
      "Root attributes: {}\n",
      "\n",
      "XML structure:\n",
      "  0: {http://www.w3.org/2005/Atom}link\n",
      "  1: {http://www.w3.org/2005/Atom}title\n",
      "  2: {http://www.w3.org/2005/Atom}id\n",
      "  3: {http://www.w3.org/2005/Atom}updated\n",
      "  4: {http://a9.com/-/spec/opensearch/1.1/}totalResults\n",
      "  5: {http://a9.com/-/spec/opensearch/1.1/}startIndex\n",
      "  6: {http://a9.com/-/spec/opensearch/1.1/}itemsPerPage\n",
      "  7: {http://www.w3.org/2005/Atom}entry\n",
      "  8: {http://www.w3.org/2005/Atom}entry\n",
      "  9: {http://www.w3.org/2005/Atom}entry\n"
     ]
    }
   ],
   "source": [
    "# Look at the raw XML response\n",
    "print(\"Raw XML response (first 1000 characters):\")\n",
    "print(response.text[:1000])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Parse the XML\n",
    "root = ET.fromstring(response.content)\n",
    "print(f\"Root tag: {root.tag}\")\n",
    "print(f\"Root attributes: {root.attrib}\")\n",
    "\n",
    "# Show the structure\n",
    "print(\"\\nXML structure:\")\n",
    "for i, child in enumerate(root):\n",
    "    print(f\"  {i}: {child.tag}\")\n",
    "    if i > 10:  # Don't print too many\n",
    "        print(\"  ...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4272bf4-ac0a-4607-bf2b-138779e196ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results available: 142084\n",
      "Number of papers returned: 3\n",
      "\n",
      "==================================================\n",
      "FIRST PAPER DETAILS:\n",
      "==================================================\n",
      "{\n",
      "  \"id\": \"http://arxiv.org/abs/2509.15217v1\",\n",
      "  \"title\": \"Generalizable Geometric Image Caption Synthesis\",\n",
      "  \"summary\": \"Multimodal large language models have various practical applications that\\ndemand strong reasoning abilities. Despite recent advancements, these models\\nstill struggle to solve complex geometric problems. A key challenge stems from\\nthe lack of high-quality image-text pair datasets for understanding geometric\\nimages. Furthermore, most template-based data synthesis pipelines typically\\nfail to generalize to questions beyond their predefined templates. In this\\npaper, we bridge this gap by introducing a complementary process of\\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\\npipeline. By adopting RLVR to refine captions for geometric images synthesized\\nfrom 50 basic geometric relations and using reward signals derived from\\nmathematical problem-solving tasks, our pipeline successfully captures the key\\nfeatures of geometry problem-solving. This enables better task generalization\\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\\nscenarios, the generated dataset enhances the general reasoning capabilities of\\nmultimodal large language models, yielding accuracy improvements of\\n$2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks\\nwith non-geometric input images of MathVista and MathVerse, along with\\n$2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks\\nin MMMU.\",\n",
      "  \"published\": \"2025-09-18T17:59:11Z\",\n",
      "  \"authors\": [\n",
      "    \"Yue Xin\",\n",
      "    \"Wenyuan Wang\",\n",
      "    \"Rui Pan\",\n",
      "    \"Ruida Wang\",\n",
      "    \"Howard Meng\",\n",
      "    \"Renjie Pi\",\n",
      "    \"Shizhe Diao\",\n",
      "    \"Tong Zhang\"\n",
      "  ],\n",
      "  \"categories\": [\n",
      "    \"cs.AI\",\n",
      "    \"cs.CV\",\n",
      "    \"cs.LG\"\n",
      "  ],\n",
      "  \"links\": {\n",
      "    \"alternate\": \"http://arxiv.org/abs/2509.15217v1\",\n",
      "    \"pdf\": \"http://arxiv.org/pdf/2509.15217v1\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Manual XML parsing to understand the structure\n",
    "namespaces = {\n",
    "    'atom': 'http://www.w3.org/2005/Atom',\n",
    "    'arxiv': 'http://arxiv.org/schemas/atom',\n",
    "    'opensearch': 'http://a9.com/-/spec/opensearch/1.1/'\n",
    "}\n",
    "\n",
    "# Get total results\n",
    "total_results = root.find('opensearch:totalResults', namespaces)\n",
    "if total_results is not None:\n",
    "    print(f\"Total results available: {total_results.text}\")\n",
    "\n",
    "# Extract paper entries\n",
    "entries = root.findall('atom:entry', namespaces)\n",
    "print(f\"Number of papers returned: {len(entries)}\")\n",
    "\n",
    "# Parse first paper in detail\n",
    "if entries:\n",
    "    first_paper = entries[0]\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FIRST PAPER DETAILS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract all the key fields\n",
    "    paper_data = {}\n",
    "    \n",
    "    # ID and URLs\n",
    "    id_elem = first_paper.find('atom:id', namespaces)\n",
    "    paper_data['id'] = id_elem.text if id_elem is not None else \"N/A\"\n",
    "    \n",
    "    # Title\n",
    "    title_elem = first_paper.find('atom:title', namespaces)\n",
    "    paper_data['title'] = title_elem.text.strip() if title_elem is not None else \"N/A\"\n",
    "    \n",
    "    # Summary (abstract)\n",
    "    summary_elem = first_paper.find('atom:summary', namespaces)\n",
    "    paper_data['summary'] = summary_elem.text.strip() if summary_elem is not None else \"N/A\"\n",
    "    \n",
    "    # Published date\n",
    "    published_elem = first_paper.find('atom:published', namespaces)\n",
    "    paper_data['published'] = published_elem.text if published_elem is not None else \"N/A\"\n",
    "    \n",
    "    # Authors\n",
    "    authors = []\n",
    "    for author in first_paper.findall('atom:author', namespaces):\n",
    "        name_elem = author.find('atom:name', namespaces)\n",
    "        if name_elem is not None:\n",
    "            authors.append(name_elem.text)\n",
    "    paper_data['authors'] = authors\n",
    "    \n",
    "    # Categories\n",
    "    categories = []\n",
    "    for category in first_paper.findall('atom:category', namespaces):\n",
    "        term = category.get('term')\n",
    "        if term:\n",
    "            categories.append(term)\n",
    "    paper_data['categories'] = categories\n",
    "    \n",
    "    # Links (especially PDF)\n",
    "    links = {}\n",
    "    for link in first_paper.findall('atom:link', namespaces):\n",
    "        rel = link.get('rel', 'alternate')\n",
    "        title = link.get('title', rel)\n",
    "        href = link.get('href')\n",
    "        links[title] = href\n",
    "    paper_data['links'] = links\n",
    "    \n",
    "    # Print extracted data\n",
    "    print(json.dumps(paper_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75cf9a4e-80dd-4d28-bde1-77c6c7af8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using arxiv library for the same search...\n",
      "Fetching results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\294774136.py:13: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3 results\n",
      "\n",
      "==================================================\n",
      "FIRST PAPER USING ARXIV LIBRARY:\n",
      "==================================================\n",
      "{\n",
      "  \"arxiv_id\": \"2509.15217v1\",\n",
      "  \"title\": \"Generalizable Geometric Image Caption Synthesis\",\n",
      "  \"summary\": \"Multimodal large language models have various practical applications that\\ndemand strong reasoning abilities. Despite recent advancements, these models\\nstill struggle to solve complex geometric problems. A key challenge stems from\\nthe lack of high-quality image-text pair datasets for understanding geometric\\nimages. Furthermore, most template-based data synthesis pipelines typically\\nfail to generalize to questions beyond their predefined templates. In this\\npaper, we bridge this gap by introducing a complementary process of\\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\\npipeline. By adopting RLVR to refine captions for geometric images synthesized\\nfrom 50 basic geometric relations and using reward signals derived from\\nmathematical problem-solving tasks, our pipeline successfully captures the key\\nfeatures of geometry problem-solving. This enables better task generalization\\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\\nscenarios, the generated dataset enhances the general reasoning capabilities of\\nmultimodal large language models, yielding accuracy improvements of\\n$2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks\\nwith non-geometric input images of MathVista and MathVerse, along with\\n$2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks\\nin MMMU.\",\n",
      "  \"published\": \"2025-09-18T17:59:11+00:00\",\n",
      "  \"updated\": \"2025-09-18T17:59:11+00:00\",\n",
      "  \"authors\": [\n",
      "    \"Yue Xin\",\n",
      "    \"Wenyuan Wang\",\n",
      "    \"Rui Pan\",\n",
      "    \"Ruida Wang\",\n",
      "    \"Howard Meng\",\n",
      "    \"Renjie Pi\",\n",
      "    \"Shizhe Diao\",\n",
      "    \"Tong Zhang\"\n",
      "  ],\n",
      "  \"primary_category\": \"cs.AI\",\n",
      "  \"categories\": [\n",
      "    \"cs.AI\",\n",
      "    \"cs.CV\",\n",
      "    \"cs.LG\"\n",
      "  ],\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2509.15217v1\",\n",
      "  \"comment\": null,\n",
      "  \"journal_ref\": null,\n",
      "  \"doi\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now let's use the arxiv library to do the same thing\n",
    "print(\"Using arxiv library for the same search...\")\n",
    "\n",
    "# Create a search\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:cs.AI\",\n",
    "    max_results=3,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "print(\"Fetching results...\")\n",
    "results = list(search.results())\n",
    "\n",
    "print(f\"Got {len(results)} results\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST PAPER USING ARXIV LIBRARY:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if results:\n",
    "    paper = results[0]\n",
    "    \n",
    "    # Create clean dictionary\n",
    "    paper_dict = {\n",
    "        'arxiv_id': paper.entry_id.split('/')[-1],  # Extract ID from URL\n",
    "        'title': paper.title,\n",
    "        'summary': paper.summary,\n",
    "        'published': paper.published.isoformat(),\n",
    "        'updated': paper.updated.isoformat() if paper.updated else None,\n",
    "        'authors': [str(author) for author in paper.authors],\n",
    "        'primary_category': paper.primary_category,\n",
    "        'categories': paper.categories,\n",
    "        'pdf_url': paper.pdf_url,\n",
    "        'comment': paper.comment,\n",
    "        'journal_ref': paper.journal_ref,\n",
    "        'doi': paper.doi\n",
    "    }\n",
    "    \n",
    "    print(json.dumps(paper_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992c9445-f5e0-4f04-a8c7-31b1d825f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEARCH: Recent AI Papers\n",
      "DESCRIPTION: All AI papers\n",
      "QUERY: cat:cs.AI\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\4096871734.py:40: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results\n",
      "\n",
      "1. Generalizable Geometric Image Caption Synthesis\n",
      "   Authors: Yue Xin, Wenyuan Wang, Rui Pan\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.AI, cs.CV, cs.LG\n",
      "   Abstract (first 100 chars): Multimodal large language models have various practical applications that\n",
      "demand strong reasoning ab...\n",
      "\n",
      "2. Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation\n",
      "   Authors: Chen Si, Qianyi Wu, Chaitanya Amballa\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.SD, cs.AI, cs.LG\n",
      "   Abstract (first 100 chars): Realistic sound simulation plays a critical role in many applications. A key\n",
      "element in sound simula...\n",
      "\n",
      "============================================================\n",
      "SEARCH: Transformer Papers\n",
      "DESCRIPTION: Papers mentioning \"transformer\"\n",
      "QUERY: all:transformer\n",
      "============================================================\n",
      "Found 2 results\n",
      "\n",
      "1. Geometric Image Synchronization with Deep Watermarking\n",
      "   Authors: Pierre Fernandez, Tomáš Souček, Nikola Jovanović\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.CV\n",
      "   Abstract (first 100 chars): Synchronization is the task of estimating and inverting geometric\n",
      "transformations (e.g., crop, rotat...\n",
      "\n",
      "2. FlowRL: Matching Reward Distributions for LLM Reasoning\n",
      "   Authors: Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.LG, cs.AI, cs.CL\n",
      "   Abstract (first 100 chars): We propose FlowRL: matching the full reward distribution via flow balancing\n",
      "instead of maximizing re...\n",
      "\n",
      "============================================================\n",
      "SEARCH: Recent ML + Attention\n",
      "DESCRIPTION: ML papers about attention\n",
      "QUERY: cat:cs.LG AND all:attention\n",
      "============================================================\n",
      "Found 2 results\n",
      "\n",
      "1. Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers\n",
      "   Authors: Andrei Chertkov, Artem Basharin, Mikhail Saygin\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.LG\n",
      "   Abstract (first 100 chars): The growing demand for energy-efficient, high-performance AI systems has led\n",
      "to increased attention ...\n",
      "\n",
      "2. Communication Efficient Split Learning of ViTs with Attention-based Double Compression\n",
      "   Authors: Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.LG, cs.AI, cs.CV\n",
      "   Abstract (first 100 chars): This paper proposes a novel communication-efficient Split Learning (SL)\n",
      "framework, named Attention-b...\n",
      "\n",
      "============================================================\n",
      "SEARCH: Recent Papers by Date\n",
      "DESCRIPTION: AI papers from last 7 days\n",
      "QUERY: cat:cs.AI AND submittedDate:[20250913 TO *]\n",
      "============================================================\n",
      "Found 0 results\n"
     ]
    }
   ],
   "source": [
    "# Let's try different types of searches\n",
    "search_queries = [\n",
    "    {\n",
    "        'name': 'Recent AI Papers',\n",
    "        'query': 'cat:cs.AI',\n",
    "        'description': 'All AI papers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Transformer Papers',\n",
    "        'query': 'all:transformer',\n",
    "        'description': 'Papers mentioning \"transformer\"'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Recent ML + Attention',\n",
    "        'query': 'cat:cs.LG AND all:attention',\n",
    "        'description': 'ML papers about attention'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Recent Papers by Date',\n",
    "        'query': f'cat:cs.AI AND submittedDate:[{(datetime.now() - timedelta(days=7)).strftime(\"%Y%m%d\")} TO *]',\n",
    "        'description': 'AI papers from last 7 days'\n",
    "    }\n",
    "]\n",
    "\n",
    "for search_config in search_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH: {search_config['name']}\")\n",
    "    print(f\"DESCRIPTION: {search_config['description']}\")\n",
    "    print(f\"QUERY: {search_config['query']}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        search = arxiv.Search(\n",
    "            query=search_config['query'],\n",
    "            max_results=2,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        \n",
    "        results = list(search.results())\n",
    "        print(f\"Found {len(results)} results\")\n",
    "        \n",
    "        for i, paper in enumerate(results):\n",
    "            print(f\"\\n{i+1}. {paper.title}\")\n",
    "            print(f\"   Authors: {', '.join(str(author) for author in paper.authors[:3])}\")\n",
    "            print(f\"   Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"   Categories: {', '.join(paper.categories[:3])}\")\n",
    "            print(f\"   Abstract (first 100 chars): {paper.summary[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5d60c5-de43-47cf-ba44-8db7f13a918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing helper functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\1930849373.py:19: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 papers:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Generalizable Geometric Image Caption Synthesis\n",
      "   ID: 2509.15217v1\n",
      "   Authors: Yue Xin, Wenyuan Wang\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.AI, cs.CV, cs.LG\n",
      "\n",
      "2. Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation\n",
      "   ID: 2509.15210v1\n",
      "   Authors: Chen Si, Qianyi Wu\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.SD, cs.AI, cs.LG\n",
      "\n",
      "3. FlowRL: Matching Reward Distributions for LLM Reasoning\n",
      "   ID: 2509.15207v1\n",
      "   Authors: Xuekai Zhu, Daixuan Cheng\n",
      "   Published: 2025-09-18\n",
      "   Categories: cs.LG, cs.AI, cs.CL\n",
      "\n",
      "\n",
      "Saved 3 papers to data/sample_papers.json\n"
     ]
    }
   ],
   "source": [
    "def search_arxiv_papers(query, max_results=5, sort_by='submittedDate'):\n",
    "    \"\"\"\n",
    "    Helper function to search arXiv and return clean data\n",
    "    \"\"\"\n",
    "    sort_criterion = {\n",
    "        'submittedDate': arxiv.SortCriterion.SubmittedDate,\n",
    "        'relevance': arxiv.SortCriterion.Relevance,\n",
    "        'lastUpdatedDate': arxiv.SortCriterion.LastUpdatedDate\n",
    "    }.get(sort_by, arxiv.SortCriterion.SubmittedDate)\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_criterion,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        paper = {\n",
    "            'arxiv_id': result.entry_id.split('/')[-1],\n",
    "            'title': result.title,\n",
    "            'summary': result.summary,\n",
    "            'published': result.published.isoformat(),\n",
    "            'authors': [str(author) for author in result.authors],\n",
    "            'categories': result.categories,\n",
    "            'pdf_url': result.pdf_url\n",
    "        }\n",
    "        papers.append(paper)\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def print_papers_summary(papers):\n",
    "    \"\"\"\n",
    "    Pretty print papers summary\n",
    "    \"\"\"\n",
    "    print(f\"Found {len(papers)} papers:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(papers, 1):\n",
    "        print(f\"{i}. {paper['title']}\")\n",
    "        print(f\"   ID: {paper['arxiv_id']}\")\n",
    "        print(f\"   Authors: {', '.join(paper['authors'][:2])}\")\n",
    "        print(f\"   Published: {paper['published'][:10]}\")\n",
    "        print(f\"   Categories: {', '.join(paper['categories'][:3])}\")\n",
    "        print()\n",
    "\n",
    "# Test the helper functions\n",
    "print(\"Testing helper functions...\")\n",
    "papers = search_arxiv_papers(\"cat:cs.AI\", max_results=3)\n",
    "print_papers_summary(papers)\n",
    "\n",
    "# Save to JSON file for later use\n",
    "with open('../data/sample_papers.json', 'w') as f:\n",
    "    json.dump(papers, f, indent=2)\n",
    "    \n",
    "print(f\"\\nSaved {len(papers)} papers to data/sample_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bd6585-33c8-4a22-be17-74efdaa04ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: cat:cs.AI\n",
      "Adding 1s delay to respect rate limits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\2664751449.py:19: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully retrieved 2 papers\n",
      "Query 'cat:cs.AI' returned 2 papers\n",
      "\n",
      "Searching for: invalid:query:format\n",
      "Adding 1s delay to respect rate limits...\n",
      "✅ Successfully retrieved 0 papers\n",
      "Query 'invalid:query:format' returned 0 papers\n",
      "\n",
      "Searching for: cat:cs.LG AND all:neural\n",
      "Adding 1s delay to respect rate limits...\n",
      "✅ Successfully retrieved 2 papers\n",
      "Query 'cat:cs.LG AND all:neural' returned 2 papers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def safe_arxiv_search(query, max_results=5, delay=1):\n",
    "    \"\"\"\n",
    "    Search with error handling and rate limiting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Searching for: {query}\")\n",
    "        print(f\"Adding {delay}s delay to respect rate limits...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "        \n",
    "        results = list(search.results())\n",
    "        print(f\"✅ Successfully retrieved {len(results)} papers\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test error handling\n",
    "test_queries = [\n",
    "    \"cat:cs.AI\",\n",
    "    \"invalid:query:format\",  # This might cause an error\n",
    "    \"cat:cs.LG AND all:neural\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    papers = safe_arxiv_search(query, max_results=2, delay=1)\n",
    "    print(f\"Query '{query}' returned {len(papers)} papers\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7a82e9-83b0-4458-afb6-889a095cc6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat:cs.LG AND all:neural'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199eb36-c327-43d7-852d-c8c7a44c223e",
   "metadata": {},
   "source": [
    "# Where to search for your keywords:\n",
    "\n",
    "* `all:transformer`     # Search title, abstract, comments, AND author names\n",
    "* `ti:attention`        # Search ONLY in title  \n",
    "* `abs:neural`          # Search ONLY in abstract\n",
    "* `co:preliminary`      # Search ONLY in comments\n",
    "* `au:lecun`           # Search ONLY in author names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ee6baa-8c9b-4ffa-b9da-94b4ae801b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_examples = [\n",
    "    {\n",
    "        'name': 'Recent AI/ML Transformers',\n",
    "        'query': '(cat:cs.AI OR cat:cs.LG) AND all:transformer',\n",
    "        'explanation': 'Transformer papers in AI or ML categories'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Recent Vision + Language',\n",
    "        'query': 'cat:cs.CV AND (all:language OR all:text OR all:caption)',\n",
    "        'explanation': 'Computer vision papers involving language'\n",
    "    },\n",
    "    {\n",
    "        'name': 'This Year NLP',\n",
    "        'query': 'cat:cs.CL AND submittedDate:[20240101 TO *]',\n",
    "        'explanation': 'NLP papers submitted in 2024'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Neural Networks NOT Deep Learning',\n",
    "        'query': 'all:neural AND all:network ANDNOT all:deep',\n",
    "        'explanation': 'Neural network papers that don\\'t mention \"deep\"'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "995da6a6-e957-43a9-b72c-b69628c06aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroai_query = \"all:neuroAI AND all:fMRI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52bf3cee-354d-4ab0-b5c3-198cac73971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: all:neuroAI AND all:fMRI\n",
      "Adding 1s delay to respect rate limits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\2664751449.py:19: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully retrieved 1 papers\n"
     ]
    }
   ],
   "source": [
    "results = safe_arxiv_search(neuroai_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ad6a89-1230-48a1-a638-850dd222959c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2409.05771v1', updated=datetime.datetime(2024, 9, 9, 16, 33, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 9, 9, 16, 33, 16, tzinfo=datetime.timezone.utc), title='Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models', authors=[arxiv.Result.Author('Emily Cheng'), arxiv.Result.Author('Richard J. Antonello')], summary='Research has repeatedly demonstrated that intermediate hidden states\\nextracted from large language models are able to predict measured brain\\nresponse to natural language stimuli. Yet, very little is known about the\\nrepresentation properties that enable this high prediction performance. Why is\\nit the intermediate layers, and not the output layers, that are most capable\\nfor this unique and highly general transfer task? In this work, we show that\\nevidence from language encoding models in fMRI supports the existence of a\\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\\nshow that this abstraction process naturally arises over the course of training\\na language model and that the first \"composition\" phase of this abstraction\\nprocess is compressed into fewer layers as training continues. Finally, we\\ndemonstrate a strong correspondence between layerwise encoding performance and\\nthe intrinsic dimensionality of representations from LLMs. We give initial\\nevidence that this correspondence primarily derives from the inherent\\ncompositionality of LLMs and not their next-word prediction properties.', comment='Equal contribution from both authors. Submitted to NeurIPS NeuroAI\\n  workshop 2024', journal_ref=None, doi=None, primary_category='cs.CL', categories=['cs.CL', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2409.05771v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2409.05771v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1825dfc-b1cc-4d1d-b758-be627bfbc48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Research has repeatedly demonstrated that intermediate hidden states\\nextracted from large language models are able to predict measured brain\\nresponse to natural language stimuli. Yet, very little is known about the\\nrepresentation properties that enable this high prediction performance. Why is\\nit the intermediate layers, and not the output layers, that are most capable\\nfor this unique and highly general transfer task? In this work, we show that\\nevidence from language encoding models in fMRI supports the existence of a\\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\\nshow that this abstraction process naturally arises over the course of training\\na language model and that the first \"composition\" phase of this abstraction\\nprocess is compressed into fewer layers as training continues. Finally, we\\ndemonstrate a strong correspondence between layerwise encoding performance and\\nthe intrinsic dimensionality of representations from LLMs. We give initial\\nevidence that this correspondence primarily derives from the inherent\\ncompositionality of LLMs and not their next-word prediction properties.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4074991-71c5-4676-a949-02fbc7530866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f464c527-bfc4-4075-8b05-39b9e43ae49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: all:fMRI\n",
      "Adding 1s delay to respect rate limits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\murph\\AppData\\Local\\Temp\\ipykernel_10620\\2664751449.py:19: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully retrieved 5 papers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2509.14994v1', updated=datetime.datetime(2025, 9, 18, 14, 27, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 18, 14, 27, 29, tzinfo=datetime.timezone.utc), title='Warp Quantification Analysis: A Framework For Path-based Signal Alignment Metrics', authors=[arxiv.Result.Author('Sir-Lord Wiafe'), arxiv.Result.Author('Vince D. Calhoun')], summary='Dynamic time warping (DTW) is widely used to align time series evolving on\\nmismatched timescales, yet most applications reduce alignment to a scalar\\ndistance. We introduce warp quantification analysis (WQA), a framework that\\nderives interpretable geometric and structural descriptors from DTW paths.\\nControlled simulations showed that each metric selectively tracked its intended\\ndriver with minimal crosstalk. Applied to large-scale fMRI, WQA revealed\\ndistinct network signatures and complementary associations with schizophrenia\\nnegative symptom severity, capturing clinically meaningful variability beyond\\nDTW distance. WQA transforms DTW from a single-score method into a family of\\nalignment descriptors, offering a principled and generalizable extension for\\nricher characterization of temporal coupling across domains where nonlinear\\nnormalization is essential.', comment='5 pages, 3 figures, conference', journal_ref=None, doi=None, primary_category='cs.CE', categories=['cs.CE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.14994v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.14994v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.14965v1', updated=datetime.datetime(2025, 9, 18, 13, 55, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 18, 13, 55, 2, tzinfo=datetime.timezone.utc), title='Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis', authors=[arxiv.Result.Author('Junhao Jia'), arxiv.Result.Author('Yunyou Liu'), arxiv.Result.Author('Cheng Yang'), arxiv.Result.Author('Yifei Sun'), arxiv.Result.Author('Feiwei Qin'), arxiv.Result.Author('Changmiao Wang'), arxiv.Result.Author('Yong Peng')], summary=\"Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive\\nwindow into the brain's functional organization by generating complex\\nfunctional networks, typically modeled as graphs. These brain networks exhibit\\na hierarchical topology that is crucial for cognitive processing. However, due\\nto inherent spatial constraints, standard Euclidean GNNs struggle to represent\\nthese hierarchical structures without high distortion, limiting their clinical\\nperformance. To address this limitation, we propose Brain-HGCN, a geometric\\ndeep learning framework based on hyperbolic geometry, which leverages the\\nintrinsic property of negatively curved space to model the brain's network\\nhierarchy with high fidelity. Grounded in the Lorentz model, our model employs\\na novel hyperbolic graph attention layer with a signed aggregation mechanism to\\ndistinctly process excitatory and inhibitory connections, ultimately learning\\nrobust graph-level representations via a geometrically sound Fr\\\\'echet mean for\\ngraph readout. Experiments on two large-scale fMRI datasets for psychiatric\\ndisorder classification demonstrate that our approach significantly outperforms\\na wide range of state-of-the-art Euclidean baselines. This work pioneers a new\\ngeometric deep learning paradigm for fMRI analysis, highlighting the immense\\npotential of hyperbolic GNNs in the field of computational psychiatry.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.14965v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.14965v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.14634v1', updated=datetime.datetime(2025, 9, 18, 5, 23, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 18, 5, 23, 5, tzinfo=datetime.timezone.utc), title=\"Extracting Interpretable Higher-Order Topological Features across Multiple Scales for Alzheimer's Disease Classification\", authors=[arxiv.Result.Author('Dengyi Zhao'), arxiv.Result.Author('Shanyong Li'), arxiv.Result.Author('Yunping Wang'), arxiv.Result.Author('Chenfei Wang'), arxiv.Result.Author('Zhiheng Zhou'), arxiv.Result.Author('Guiying Yan'), arxiv.Result.Author('Xingqin Qi')], summary=\"Brain network topology, derived from functional magnetic resonance imaging\\n(fMRI), holds promise for improving Alzheimer's disease (AD) diagnosis. Current\\nmethods primarily focus on lower-order topological features, often overlooking\\nthe significance of higher-order features such as connected components, cycles,\\nand cavities. These higher-order features are critical for understanding normal\\nbrain function and have been increasingly linked to the pathological mechanisms\\nof AD. However, their quantification for diagnosing AD is hindered by their\\ninherent nonlinearity and stochasticity in the brain. This paper introduces a\\nnovel framework for diagnosing Alzheimer's disease that uses persistent\\nhomology to extract higher-order topological features from fMRI data. It also\\nintroduces four quantitative methods that capture subtle, multiscale geometric\\nvariations in functional brain networks associated with AD. Our experimental\\nresults demonstrate that this framework significantly outperforms existing\\nmethods in AD classification. Extensive ablation studies and interpretability\\nanalysis confirm the effectiveness of our framework. Our study also reveals\\nthat the number of cycles or cavities significantly decrease in AD patients.\\nThe extracted key brain regions derived from cycles and cavities align with\\ndomain knowledge in neuroscience literature and provide direct and insightful\\nfindings. This study highlights the potential of higher-order topological\\nfeatures for early AD detection and significantly advances the field of brain\\ntopology analysis in neurodegenerative disease research.\", comment=None, journal_ref=None, doi=None, primary_category='math.GT', categories=['math.GT'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.14634v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.14634v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.14577v1', updated=datetime.datetime(2025, 9, 18, 3, 26, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 18, 3, 26, 9, tzinfo=datetime.timezone.utc), title='Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition', authors=[arxiv.Result.Author('Yang Xu'), arxiv.Result.Author('Junpeng Li'), arxiv.Result.Author('Changchun Hua'), arxiv.Result.Author('Yana Yang')], summary=\"The Large Margin Distribution Machine (LMDM) is a recent advancement in\\nclassifier design that optimizes not just the minimum margin (as in SVM) but\\nthe entire margin distribution, thereby improving generalization. However,\\nexisting LMDM formulations are limited to vectorized inputs and struggle with\\nhigh-dimensional tensor data due to the need for flattening, which destroys the\\ndata's inherent multi-mode structure and increases computational burden. In\\nthis paper, we propose a Structure-Preserving Margin Distribution Learning for\\nHigh-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates\\ndirectly on tensor representations without vectorization. The SPMD-LRT\\npreserves multi-dimensional spatial structure by incorporating first-order and\\nsecond-order tensor statistics (margin mean and variance) into the objective,\\nand it leverages low-rank tensor decomposition techniques including rank-1(CP),\\nhigher-rank CP, and Tucker decomposition to parameterize the weight tensor. An\\nalternating optimization (double-gradient descent) algorithm is developed to\\nefficiently solve the SPMD-LRT, iteratively updating factor matrices and core\\ntensor. This approach enables SPMD-LRT to maintain the structural information\\nof high-order data while optimizing margin distribution for improved\\nclassification. Extensive experiments on diverse datasets (including MNIST,\\nimages and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior\\nclassification accuracy compared to conventional SVM, vector-based LMDM, and\\nprior tensor-based SVM extensions (Support Tensor Machines and Support Tucker\\nMachines). Notably, SPMD-LRT with Tucker decomposition attains the highest\\naccuracy, highlighting the benefit of structure preservation. These results\\nconfirm the effectiveness and robustness of SPMD-LRT in handling\\nhigh-dimensional tensor data for classification.\", comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.14577v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.14577v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2509.13612v1', updated=datetime.datetime(2025, 9, 17, 1, 8, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2025, 9, 17, 1, 8, 3, tzinfo=datetime.timezone.utc), title='Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans', authors=[arxiv.Result.Author('Chuyang Zhou'), arxiv.Result.Author('Ziao Ji'), arxiv.Result.Author('Daochang Liu'), arxiv.Result.Author('Dongang Wang'), arxiv.Result.Author('Chenyu Wang'), arxiv.Result.Author('Chang Xu')], summary=\"Understanding how spontaneous brain activity relates to stimulus-driven\\nneural responses is a fundamental challenge in cognitive neuroscience. While\\ntask-based functional magnetic resonance imaging (fMRI) captures localized\\nstimulus-evoked brain activation, its acquisition is costly, time-consuming,\\nand difficult to scale across populations. In contrast, resting-state fMRI\\n(rs-fMRI) is task-free and abundant, but lacks direct interpretability. We\\nintroduce Rest2Visual, a conditional generative model that predicts visually\\nevoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It\\nfollows a volumetric encoder--decoder design, where multiscale 3D features from\\nrs-fMRI are modulated by image embeddings via adaptive normalization, enabling\\nspatially accurate, stimulus-specific activation synthesis. To enable model\\ntraining, we construct a large-scale triplet dataset from the Natural Scenes\\nDataset (NSD), aligning each rs-fMRI volume with stimulus images and their\\ncorresponding ve-fMRI activation maps. Quantitative evaluation shows that the\\npredicted activations closely match ground truth across standard similarity and\\nrepresentational metrics, and support successful image reconstruction in\\ndownstream decoding. Notably, the predicted maps preserve subject-specific\\nstructure, demonstrating the model's capacity to generate individualized\\nfunctional surrogates. Our results provide compelling evidence that\\nindividualized spontaneous neural activity can be transformed into\\nstimulus-aligned representations, opening new avenues for scalable, task-free\\nfunctional brain modeling.\", comment=None, journal_ref=None, doi=None, primary_category='q-bio.NC', categories=['q-bio.NC', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2509.13612v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2509.13612v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_arxiv_search('all:fMRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b81f515-8fcc-4c4e-97fc-de7538456115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
